{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57610618",
   "metadata": {},
   "source": [
    "# Agenda\n",
    "\n",
    "1. [Introduction. What is MLOps?](#Intro)\n",
    "2. [Model Compression](#Compression)\n",
    "3. [Deployment. Simple Approach. BentoML](#BentoML)\n",
    "4. [Deployment. Advanced Approach. Custom Services](#Custom)\n",
    "5. [Homework](#Homework)\n",
    "\n",
    "<a id='Intro'></a>\n",
    "# Introduction. What is MLOps?\n",
    "MLOps stands for Machine Learning Operations. MLOps is a core function of Machine Learning engineering, focused on streamlining the process of taking machine learning models to production, and then maintaining and monitoring them. MLOps is a collaborative function, often comprising data scientists, devops engineers, and IT. <br>\n",
    "![](https://cms.databricks.com/sites/default/files/inline-images/mlops-components.png) <br>\n",
    "In this lecture our main focus will be on model inference / model deployments steps.\n",
    "\n",
    "<a id='Compression'></a>\n",
    "# Model Compression\n",
    "The primary benefit of compression involves reduced compute costs during inference: The computational resource reduction is the primary motivator for performing model compression. Model compression reduces CPU/GPU time, memory usage, and disk storage. It can make a model suitable for production that would have previously been too expensive, too slow, or too large. <br>\n",
    "\n",
    "Even when it’s beneficial, compression is not free. Costs of implementing it include:\n",
    "\n",
    "- Increased deployment complexity: After implementing various model compression techniques there is more to keep track of, namely the original trained model and the compressed models. We must choose the model to deploy and spend time making this choice.\n",
    "- Decreased accuracy: Some model compression techniques result in a loss of accuracy (however this is measured). This cost has an obvious counterpart in that the benefits of the model compression technique may outweigh the accuracy loss.\n",
    "- Compute cost: While model compression reduces the compute resources required for inference, the compression itself may be computationally expensive to perform. Notably, distillation introduces an additional iterative training step.\n",
    "- Your time: Adding a step to the lifecycle requires an investment of your time.\n",
    "\n",
    "> **TODO**: You can read more about compression [here](https://medium.com/data-science-at-microsoft/model-compression-and-optimization-why-think-bigger-when-you-can-think-smaller-216ec096f68b).\n",
    "\n",
    "## ONNX conversion and ONNX Runtime\n",
    "ONNX is an open format that is used to represent various Machine Learning models. It works by defining a common set of operators and a common file format to enable data scientists to use models in a wide variety of frameworks. The conversion process for natural language models from (insert your favorite neural network library here) to ONNX additionally functions as a model compression technique. This is because the operators defined by ONNX have been optimized for specific types of hardware, resulting in slightly smaller models.<br>\n",
    "\n",
    "\n",
    "The true utility of ONNX comes in the form of the ONNX Runtime backend. One of the optimizations with the most impact that ONNX Runtime implements is the capacity to “fuse” operations and activations within a model. The result of this fusion is a significant reduction in memory footprint and calculations per inference. For popular NLP model families, there exists customized logic to identify the operations within the models that can be fused."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19309b6f",
   "metadata": {},
   "source": [
    "pip install onnx==1.14.1\n",
    "pip install onnxruntime\n",
    "pip install optimum[onnxruntime]==1.13.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e85940c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import onnxruntime as ort\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60c1dbcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "onnx_path = './bert_base_uncased.onnx'\n",
    "\n",
    "dummy_input = tokenizer('here is the sample text for the dummy input', return_tensors=\"pt\", max_length=50, padding='max_length')\n",
    "\n",
    "torch.onnx.export(\n",
    "    model=model,\n",
    "    args=(dummy_input['input_ids'].to(device), dummy_input['token_type_ids'].to(device), dummy_input['attention_mask'].to(device)),\n",
    "    f=onnx_path,\n",
    "    export_params=True,\n",
    "    opset_version=11,\n",
    "    do_constant_folding=True,\n",
    "    input_names=['input_ids', 'token_type_ids', 'attention_mask'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes={\n",
    "        'input_ids': {0: 'batch_size'},\n",
    "        'token_type_ids': {0: 'batch_size'},\n",
    "        'attention_mask': {0: 'batch_size'},\n",
    "        'output': {0: 'batch_size'}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd8975c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model Inference Time: 11.530035018920898 seconds\n",
      "ONNX Model Inference Time: 7.327675104141235 seconds\n",
      "Inference acceleration is 1.57x times\n"
     ]
    }
   ],
   "source": [
    "texts = 3*[\n",
    "    \"The road whispers secrets to those who travel without direction.\",\n",
    "    \"In the heart of the city, the jazz soul dances with shadows.\",\n",
    "    \"Under the moon's gaze, we found stories waiting in every corner.\",\n",
    "    \"The rhythm of the night was our compass through uncharted dreams.\",\n",
    "    \"With each mile, the horizon whispered promises of freedom.\",\n",
    "    \"In the silence of the mountains, our laughter echoed like ancient songs.\",\n",
    "    \"We were nomads of the twilight, seeking truth in the stars.\",\n",
    "    \"Every sunset was a painting, a masterpiece of our wanderlust.\",\n",
    "    \"The neon lights flickered, writing poetry in the dark.\",\n",
    "    \"Our conversations were a patchwork of memories and musings.\",\n",
    "    \"In the arms of the wilderness, we found our untamed spirits.\",\n",
    "    \"The city slept, but we walked its dreams.\",\n",
    "    \"With the dawn came clarity, like the first breath of a new world.\",\n",
    "    \"Our hearts beat to the rhythm of the train's song.\",\n",
    "    \"In the quiet cafes, we sipped on stories and coffee.\",\n",
    "    \"The road was a canvas, and our journey, its art.\",\n",
    "    \"Night skies told tales older than time in their starry script.\",\n",
    "    \"We found solace in the symphony of the wind and waves.\",\n",
    "    \"Each town held a secret, whispered in the rustling leaves.\",\n",
    "    \"We danced with shadows, embracing the mystery of the night.\",\n",
    "    \"The mountains stood as guardians of our deepest thoughts.\",\n",
    "    \"In the flicker of the campfire, our dreams danced freely.\",\n",
    "    \"The desert's vastness echoed our longing for the unknown.\",\n",
    "    \"Our path was lit by the hopes of a thousand adventures.\",\n",
    "    \"In the depths of the forest, time stood still, a silent witness.\",\n",
    "    \"We followed the river's song, meandering through forgotten lands.\",\n",
    "    \"The city's heartbeat was a melody of chaos and beauty.\",\n",
    "    \"Under the starlit sky, our souls whispered tales of old.\",\n",
    "    \"The open road was our teacher, life its lesson.\",\n",
    "    \"In every journey's end, a new story was born.\",\n",
    "    \"We were seekers of the dawn, chasing the first light.\",\n",
    "    \"The rain's rhythm spoke of journeys yet to come.\",\n",
    "    \"Our map was drawn in dreams and detours.\",\n",
    "    \"In the twilight, the world seemed to pause, listening to our footsteps.\",\n",
    "    \"We wandered through the pages of the earth, writing our story.\",\n",
    "    \"The ocean's vastness mirrored our boundless curiosity.\",\n",
    "    \"In the stillness of the night, every star held a wish.\",\n",
    "    \"Our laughter was the soundtrack of endless roads.\",\n",
    "    \"We found poetry in the ordinary, magic in the mundane.\",\n",
    "    \"Every mile traveled was a verse in our epic.\",\n",
    "    \"The whispers of the wind were our guide through the unknown.\",\n",
    "    \"In the heart of the forest, we spoke the language of the wild.\",\n",
    "    \"The city at dawn was a canvas of hushed possibilities.\",\n",
    "    \"Our journey was a mosaic of moments, each a priceless gem.\",\n",
    "    \"With each setting sun, our stories grew richer.\",\n",
    "    \"The mountains called to us, their peaks like beckoning fingers.\",\n",
    "    \"In the quiet of the countryside, our thoughts found voice.\",\n",
    "    \"We were pilgrims of the moonlight, worshiping the night.\",\n",
    "    \"The road's end was not a destination, but a new beginning.\",\n",
    "    \"In the labyrinth of streets, we found pieces of ourselves.\"\n",
    "]\n",
    "\n",
    "# Measure inference time\n",
    "start_time = time.time()\n",
    "for text in texts:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=50, padding='max_length')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "end_time = time.time()\n",
    "\n",
    "original_model_time = end_time - start_time\n",
    "print(f\"Original Model Inference Time: {original_model_time} seconds\")\n",
    "\n",
    "# Load the ONNX model\n",
    "session = ort.InferenceSession(\"bert_base_uncased.onnx\")\n",
    "\n",
    "# Measure inference time\n",
    "start_time = time.time()\n",
    "for text in texts:\n",
    "    inputs = tokenizer(text, return_tensors=\"np\", max_length=50, padding='max_length')\n",
    "    inputs_onnx = {k: v for k, v in inputs.items()}\n",
    "    outputs = session.run(None, inputs_onnx)\n",
    "end_time = time.time()\n",
    "\n",
    "onnx_model_time = end_time - start_time\n",
    "print(f\"ONNX Model Inference Time: {onnx_model_time} seconds\")\n",
    "print(f\"Inference acceleration is {round(original_model_time/onnx_model_time, 2)}x times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e782b8",
   "metadata": {},
   "source": [
    "## Quantization\n",
    "\n",
    "Our next approach, quantization, is the process of mapping values from a large set to a smaller set. Rounding and truncation are both basic examples of quantization but aren’t how quantization manifests in the realm of neural networks.\n",
    "\n",
    "\n",
    "Neural nets, in most default configurations, have weights stored as 32-bit floating point numbers (fp32). Operations with fp32 numbers are expensive and most hardware is not optimized to compute with them.\n",
    "\n",
    "\n",
    "The most common quantization process takes fp32 numbers and reduces them to 8-bit integers (int8). The result is a model with a quarter the size that can perform inference at nearly four times the original speed. These benefits are at the cost of a loss in precision in the output of the model. Whether this loss in precision affects the target metric for the model is task and model dependent. Typically, when models have discrete outputs, such as identification of a handwritten digit, this precision loss has less effect.\n",
    "\n",
    "\n",
    "This form of quantization comes with a catch. Moving from fp32 to int8 is most beneficial for models inferencing on the CPU.\n",
    "\n",
    "![](https://developer-blogs.nvidia.com/wp-content/uploads/2021/07/qat-training-precision.png)\n",
    "![](https://developer-blogs.nvidia.com/wp-content/uploads/2021/07/8-bit-signed-integer-quantization.png)\n",
    "\n",
    "> **TODO**: You can read more about the exact process details [here](https://developer.nvidia.com/blog/achieving-fp32-accuracy-for-int8-inference-using-quantization-aware-training-with-tensorrt/) and get more practical transformers examples [here](https://github.com/ELS-RD/transformer-deploy/blob/main/demo/quantization/quantization_end_to_end.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "220ab183",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.0/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.0/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.1/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.1/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.2/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.2/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.3/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.3/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.4/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.4/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.5/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.5/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.6/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.6/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.7/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.7/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.8/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.8/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.9/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.9/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.10/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.10/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.11/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.11/attention/self/MatMul_1]\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "quantize_dynamic(\"bert_base_uncased.onnx\", \"bert_base_uncased_quant.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e92d5e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Quantized Model Inference Time: 5.012095624750311 seconds\n",
      "Inference acceleration is 2.3x times\n"
     ]
    }
   ],
   "source": [
    "# Load the ONNX model\n",
    "session = ort.InferenceSession(\"bert_base_uncased_quant.onnx\")\n",
    "\n",
    "# Measure inference time\n",
    "start_time = time.time()\n",
    "for text in texts:\n",
    "    inputs = tokenizer(text, return_tensors=\"np\", max_length=50, padding='max_length')\n",
    "    inputs_onnx = {k: v for k, v in inputs.items()}\n",
    "    outputs = session.run(None, inputs_onnx)\n",
    "end_time = time.time()\n",
    "\n",
    "onnx_model_time_quant = end_time - start_time\n",
    "print(f\"ONNX Quantized Model Inference Time: {onnx_model_time_quant} seconds\")\n",
    "print(f\"Inference acceleration is {round(original_model_time/onnx_model_time_quant, 2)}x times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eabde5",
   "metadata": {},
   "source": [
    "## Distillation\n",
    "![](https://i0.wp.com/www.merchantnavydecoded.com/wp-content/uploads/2023/06/steam-egine-26.png?fit=1140%2C570&ssl=1)\n",
    "Distillation is one of the most powerful approaches when it comes to model compression. Implementing a state-of-the-art distillation process can cut your model size down by a factor of seven, increase inference speeds by a factor of ten, and have almost no effect on the model’s accuracy metric (e.g. distilBERT tinyBERT).\n",
    "\n",
    "\n",
    "Here is more good news: Distillation is still fairly young! There are likely many improvements to come. Now for some bad news: Distillation is still fairly young! This means that the process is not yet widely implemented in standard libraries. Research code does exist that can be used to distill various model architectures (such as BERT, GPT2, and BART), though to implement distillation on a custom model it is necessary to understand the full process.\n",
    "\n",
    "> **TODO**: You can read more about the exact process details [here](https://medium.com/p/dd4973dbc764)\n",
    "\n",
    "Teacher Student networks — How do they exactly work?\n",
    "- Train the Teacher Network : The highly complex teacher network is first trained separately using the complete dataset. This step requires high computational performance and thus can only be done offline (on high performing GPUs).\n",
    "- Establish Correspondence : While designing a student network, a correspondence needs to be established between intermediate outputs of the student network and the teacher network. This correspondence can involve directly passing the output of a layer in the teacher network to the student network, or performing some data augmentation before passing it to the student network. The way the knowledge of the good answers is transferred to the Student is through the loss function. Essentially, we want to train the Student so that it mimics the same distribution that the Teacher provides. To do this, we must also understand what the Student outputs are before it is even trained. This measurement is called the Kullback-Leibler, or KL, divergence. This approximates the work it takes to turn the red curve into the blue curve. The result is a loss function that has a term measuring the KL divergence between the Student distribution and the Teacher distribution.\n",
    "\n",
    "![](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Knowledge-Distillation_4.png?resize=900%2C356&ssl=1)\n",
    "- Forward Pass through the Teacher network : Pass the data through the teacher network to get all intermediate outputs and then apply data augmentation (if any) to the same.\n",
    "- Backpropagation through the Student Network : Now use the outputs from the teacher network and the correspondence relation to backpropagate error in the student network, so that the student network can learn to replicate the behavior of the teacher network. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c2ff9f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distilled Model Inference Time: 5.7951743602752686 seconds\n",
      "Inference acceleration is 1.99x times\n"
     ]
    }
   ],
   "source": [
    "model_name = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "# Measure inference time\n",
    "start_time = time.time()\n",
    "for text in texts:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=50, padding='max_length')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "end_time = time.time()\n",
    "\n",
    "distilled_model_time = end_time - start_time\n",
    "print(f\"Distilled Model Inference Time: {distilled_model_time} seconds\")\n",
    "print(f\"Inference acceleration is {round(original_model_time/distilled_model_time, 2)}x times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70db8266",
   "metadata": {},
   "source": [
    "## Distillation + ONNX + Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54e8d00b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abazdyrev/anaconda3/lib/python3.11/site-packages/transformers/models/distilbert/modeling_distilbert.py:223: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  mask, torch.tensor(torch.finfo(scores.dtype).min)\n",
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.0/attention/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.0/attention/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.1/attention/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.1/attention/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.2/attention/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.2/attention/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.3/attention/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.3/attention/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.4/attention/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.4/attention/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.5/attention/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.5/attention/MatMul_1]\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "onnx_path = './distilbert_base_uncased.onnx'\n",
    "\n",
    "dummy_input = tokenizer('here is the sample text for the dummy input', return_tensors=\"pt\", max_length=50, padding='max_length')\n",
    "\n",
    "torch.onnx.export(\n",
    "    model=model,\n",
    "    args=(dummy_input['input_ids'].to(device), dummy_input['attention_mask'].to(device)),\n",
    "    f=onnx_path,\n",
    "    export_params=True,\n",
    "    opset_version=11,\n",
    "    do_constant_folding=True,\n",
    "    input_names=['input_ids', 'attention_mask'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes={\n",
    "        'input_ids': {0: 'batch_size'},\n",
    "        'attention_mask': {0: 'batch_size'},\n",
    "        'output': {0: 'batch_size'}\n",
    "    }\n",
    ")\n",
    "\n",
    "quantize_dynamic(onnx_path, './distilbert_base_uncased_quant.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4ca2338",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Distilled Quantized Model Inference Time: 2.8224806785583496 seconds\n",
      "Inference acceleration is 4.09x times\n"
     ]
    }
   ],
   "source": [
    "# Load the ONNX model\n",
    "session = ort.InferenceSession('distilbert_base_uncased_quant.onnx')\n",
    "\n",
    "# Measure inference time\n",
    "start_time = time.time()\n",
    "for text in texts:\n",
    "    inputs = tokenizer(text, return_tensors=\"np\", max_length=50, padding='max_length')\n",
    "    inputs_onnx = {k: v for k, v in inputs.items()}\n",
    "    outputs = session.run(None, inputs_onnx)\n",
    "end_time = time.time()\n",
    "\n",
    "onnx_model_time_quant_distilled = end_time - start_time\n",
    "print(f\"ONNX Distilled Quantized Model Inference Time: {onnx_model_time_quant_distilled} seconds\")\n",
    "print(f\"Inference acceleration is {round(original_model_time/onnx_model_time_quant_distilled, 2)}x times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da320e51",
   "metadata": {},
   "source": [
    "<a id='BentoML'></a>\n",
    "# Deployment. Simple Approach. BentoML\n",
    "\n",
    "ML model deployment is the process of integrating a trained ML model into an existing production environment to make practical, actionable decisions based on new data. It's a crucial step in a machine learning project as it allows the model to provide real-world value. Here we will consider **Real-time Inference** - for applications requiring immediate feedback, models are deployed in an environment that supports real-time data processing with a simple implementation using BentoML.\n",
    "\n",
    "**BentoML** is designed for teams working to bring machine learning (ML) models into production in a reliable, scalable, and cost-efficient way. In particular, AI application developers can leverage BentoML to easily integrate state-of-the-art pre-trained models into their applications. By seamlessly bridging the gap between model creation and production deployment, BentoML promotes collaboration between developers and in-house data science teams.\n",
    "\n",
    "> **TODO**: read documentations and do some experiments with more advanced options described [here](https://docs.bentoml.org/en/latest/quickstarts/deploy-a-transformer-model-with-bentoml.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f4e0de-df73-4c29-8a5c-c178f66543b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install bentoml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96fd8dd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(tag=\"text-classification-pipe:auqktued2osicqqb\", path=\"/home/abazdyrev/bentoml/models/text-classification-pipe/auqktued2osicqqb/\")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bentoml\n",
    "import transformers\n",
    "\n",
    "pipe = transformers.pipeline(\"text-classification\", device='cpu')\n",
    "\n",
    "bentoml.transformers.save_model(\n",
    "  \"text-classification-pipe\",\n",
    "  pipe,\n",
    "  signatures={\n",
    "    \"__call__\": {\"batchable\": True}  # Enable dynamic batching for model\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bfc92f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[1m \u001b[0m\u001b[1mTag                    \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mModule              \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mSize      \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mCreation Time      \u001b[0m\u001b[1m \u001b[0m\n",
      " text-classification-pi…  bentoml.transformers  256.35 MiB  2023-11-15 14:57:05 \n"
     ]
    }
   ],
   "source": [
    "!bentoml models list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7f26569",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2023-11-15T14:58:07+0000 [INFO] [cli] Environ for worker 0: set CPU thread count to 8\n",
      "2023-11-15T14:58:07+0000 [INFO] [cli] Prometheus metrics for HTTP BentoServer from \"service.py:svc\" can be accessed at http://localhost:3000/metrics.\n",
      "2023-11-15T14:58:07+0000 [INFO] [cli] Starting production HTTP BentoServer from \"service.py:svc\" listening on http://0.0.0.0:3000 (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "!bentoml serve bentoml_service.py:svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99c29e5e-a795-45de-bcd7-bf7cbdec687b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"label\":\"POSITIVE\",\"score\":0.9998418092727661}'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "requests.post('http://localhost:3000/classify', data=\"BentoML is awesome\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "646f63df-d8aa-4b43-941d-efbcc9103f60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"label\":\"NEGATIVE\",\"score\":0.9997016787528992}'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "requests.post('http://localhost:3000/classify', data=\"ML deployment is a very complicated and awful process\").text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3514b0-0596-4bfe-b334-8e5a280508f2",
   "metadata": {},
   "source": [
    "<a id='Custom'></a>\n",
    "# Deployment. Advanced Approach. Custom Services\n",
    "\n",
    "## Docker \n",
    "Docker is a platform that uses containerization technology to make it easier to create, deploy, and run applications. Containers can be thought of as a kind of lightweight, standalone, and executable package of software that includes everything needed to run an application: code, runtime, system tools, system libraries, and settings.\n",
    "Here are some key aspects of Docker:\n",
    "- Containers: Docker containers wrap a piece of software in a complete filesystem that contains everything needed to run: code, runtime, system tools, and libraries. This guarantees that the software will always run the same, regardless of its environment.\n",
    "- Isolation: Containers are isolated from each other and the host system. They have their own filesystem, their own networking, and their own isolated process space. This provides a layer of security and allows multiple containers to run on the same host machine without interference.\n",
    "- Portability: Since Docker containers contain everything needed to run an application, they are highly portable. You can run these containers on any machine that has Docker installed, regardless of the underlying operating system.\n",
    "- Consistency: Docker provides a consistent environment for development, testing, and production. This consistency helps to reduce the \"it works on my machine\" problem when working in teams.\n",
    "- Microservices Architecture: Docker is well-suited for microservices architecture, where complex applications are broken down into smaller, independent services. This makes it easier to update and scale applications.\n",
    "- Dockerfiles: Docker uses a simple text file called a Dockerfile to automate the building of container images. A Dockerfile specifies all the steps that need to be taken to create the image.\n",
    "- CI/CD Integration: Docker integrates well with continuous integration and continuous deployment (CI/CD) workflows, making it easier to automate the testing and deployment of applications.\n",
    "\n",
    "![](https://docs.docker.com/get-started/images/docker-architecture.png)\n",
    "\n",
    "## FastAPI\n",
    "FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.7+ based on standard Python type hints. It's known for its high speed and ease of use and has been gaining popularity in the Python community. Here are some key features and benefits of FastAPI:\n",
    "- Speed: FastAPI is built on Starlette for the web parts and Pydantic for the data parts, which makes it one of the fastest Python frameworks available, only slower than NodeJS and Go according to some benchmarks.\n",
    "- Automatic Documentation: FastAPI automatically generates interactive API documentation (using Swagger UI and ReDoc) that lets you call and test your API directly from the browser.\n",
    "- Easy to Use: It has been designed to be easy to use while also ensuring that new developers can quickly understand its operation. FastAPI simplifies the process of building robust APIs.\n",
    "- Asynchronous Code Support: FastAPI is one of the few Python web frameworks to support asynchronous request handlers out of the box, making it suitable for high I/O-bound applications.\n",
    "- Security and Authentication: FastAPI includes several tools to help with API security, such as OAuth2 password flow and JWT tokens, as well as utilities for hashing passwords.\n",
    "- Extensibility: Being lightweight and based on standard Python tools, FastAPI is very easy to extend with various databases, ORMs, authentication and authorization frameworks, data validation libraries, and more.\n",
    "- Built for Production: It has features and utilities to help deploy and run your applications in production environments, including Docker integration.\n",
    "\n",
    "**FastAPI-Docker model deployment example in ./fastapi_service**\n",
    "\n",
    "To run in you need to instal `docker` and `docker-compose`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b22b0b-18b8-4a0f-874a-17aabac9dc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd fastapi_service\n",
    "!docker-compose build\n",
    "!docker-compose up -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bf810c3-5b40-4444-b02e-21113fb7affb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"label\":\"NEGATIVE\",\"score\":0.9997016787528992}'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "requests.post('http://0.0.0.0:8088/classify', data=\"ML deployment is a very complicated and awful process\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5f43c82-b3e7-46df-9899-afc72d5894d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"label\":\"POSITIVE\",\"score\":0.9997023940086365}'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "requests.post('http://0.0.0.0:8088/classify', data=\"ML deployment is a very simple and exciting process\").text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed2bca9-707c-4302-ae1f-fccc65519d03",
   "metadata": {},
   "source": [
    "<a id='Homework'></a>\n",
    "# Homework\n",
    "Theory:\n",
    "- Follow all **TODO** links\n",
    "\n",
    "Practice (15 points):\n",
    "- Implement ONNX conversion, quantization and usage of distilled versions of pretrains for your models for one of your previous tasks (tweeter disasters/UA Locations/CommonLit/your finetuned generativeAI model) and check speed improvements and metrics degradation on your local validation.\n",
    "- Deploy your model (optimized with onnx/distillation/quantization) localy using Bento (or similar) tools or using custom approach with FastAPI/Flask and Docker.\n",
    "- (Advanced) Deploy your model in a cloud service e.g. using GCP free credits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
